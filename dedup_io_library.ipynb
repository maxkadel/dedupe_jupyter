{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a675da0-bee4-411e-8e4d-eed1256b8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import optparse\n",
    "import os\n",
    "import re\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d15e5e9-bb1e-4260-afbd-bdaf2664da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    column = unidecode(column)\n",
    "    column = re.sub(\"  +\", \" \", column)\n",
    "    column = re.sub(\"\\n\", \" \", column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "# If data is missing, indicate that by setting the value to None\n",
    "\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6bcb595-0a0e-433e-88f3-10936876a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our data from a CSV file and create a dictionary of records, where the key is a unique record ID and each value is dict\n",
    "def readData(filename):\n",
    "    data_d = {}\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "            row_id = row[\"ID\"]\n",
    "            data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed638d2-0911-4dad-b1c6-e01708b89a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb84278-a703-409f-83bb-b94ebe97d389",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n"
     ]
    }
   ],
   "source": [
    "# input_file = \"science_with_duplicates.csv\"\n",
    "input_file = \"combined_with_duplicates.csv\"\n",
    "output_file = \"csv_example_output_combined.csv\"\n",
    "settings_file = \"combined_csv_example_learned_settings\"\n",
    "training_file = \"combined_csv_example_training.json\"\n",
    "\n",
    "print(\"importing data ...\")\n",
    "data_d = readData(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a88a986-4357-4344-9989-11ef32a31072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from combined_csv_example_learned_settings\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(settings_file):\n",
    "    print(\"reading from\", settings_file)\n",
    "    with open(settings_file, \"rb\") as f:\n",
    "        deduper = dedupe.StaticDedupe(f)\n",
    "else:\n",
    "    fields = [\n",
    "                dedupe.variables.String(\"Title\"),\n",
    "                dedupe.variables.String(\"Author\", has_missing=True),\n",
    "                dedupe.variables.String(\"Format\", has_missing=True),\n",
    "                dedupe.variables.String(\"Language\"),\n",
    "                dedupe.variables.String(\"Published/Created\", has_missing=True),\n",
    "                dedupe.variables.String(\"Date\", has_missing=True),\n",
    "                dedupe.variables.String(\"Description\"),\n",
    "                dedupe.variables.String(\"Edition\", has_missing=True)\n",
    "            ]\n",
    "    # Create a new deduper object and pass our data model to it.\n",
    "\n",
    "    deduper = dedupe.Dedupe(fields)\n",
    "\n",
    "    if os.path.exists(training_file):\n",
    "        print(\"reading labeled examples from \", training_file)\n",
    "        with open(training_file, \"rb\") as f:\n",
    "            deduper.prepare_training(data_d, f)\n",
    "    else:\n",
    "        deduper.prepare_training(data_d)\n",
    "\n",
    "        print(\"starting active labeling...\")\n",
    "\n",
    "        dedupe.console_label(deduper)\n",
    "        \n",
    "        # Using the examples we just labeled, train the deduper and learn blocking predicates\n",
    "        deduper.train()\n",
    "        \n",
    "        # When finished, save our training to disk\n",
    "        with open(training_file, \"w\") as tf:\n",
    "            deduper.write_training(tf)\n",
    "\n",
    "        # Save our weights and predicates to disk. If the settings file exists, we will skip all the training and learning next time we run this file.\n",
    "        with open(settings_file, \"wb\") as sf:\n",
    "            deduper.write_settings(sf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a909e9b2-b8a6-4f77-95c3-fecbb1fe8481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering...\n",
      "# duplicate sets 143\n"
     ]
    }
   ],
   "source": [
    "        print(\"clustering...\")\n",
    "        clustered_dupes = deduper.partition(data_d, 0.5)\n",
    "        \n",
    "        print(\"# duplicate sets\", len(clustered_dupes))\n",
    "        # Writing Results\n",
    "        # Write our original data back out to a CSV with a new column called ‘Cluster ID’ which indicates which records refer to each other.\n",
    "        cluster_membership = {}\n",
    "        for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "            for record_id, score in zip(records, scores):\n",
    "                cluster_membership[record_id] = {\n",
    "                    \"Cluster ID\": cluster_id,\n",
    "                    \"confidence_score\": score,\n",
    "                }\n",
    "        \n",
    "        with open(output_file, \"w\") as f_output, open(input_file) as f_input:\n",
    "        \n",
    "            reader = csv.DictReader(f_input)\n",
    "            fieldnames = [\"Cluster ID\", \"confidence_score\"] + reader.fieldnames\n",
    "        \n",
    "            writer = csv.DictWriter(f_output, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "        \n",
    "            for row in reader:\n",
    "                row_id = row[\"ID\"]\n",
    "                row.update(cluster_membership[row_id])\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc2195-064b-433c-8e40-4f9aa81596bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
